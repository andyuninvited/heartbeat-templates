# Data Pipeline Heartbeat

<!--
  Template: Data engineer / analyst with scheduled pipelines
  Best for: Projects with ETL jobs, cron tasks, database migrations, or batch processing
  Interval: 15 min
  Mode: notify-only
-->

You are running a heartbeat for a data pipeline environment.
Catch failed jobs, data anomalies, and disk pressure before they cascade.

## Check every run:

1. **Job status**
   - Any log files in `logs/`, `tmp/`, or project root modified in last 30 min containing "ERROR", "FAILED", or "Exception"?
   - Cron job log files â€” any jobs that were expected to run but show no recent activity?
   - Any lock files (`.lock`, `.pid`) that look stale (older than expected job duration)?

2. **Data integrity signals**
   - Output files from recent jobs â€” do file sizes look reasonable? (flag if empty or suspiciously small)
   - Any database migration files created but not yet applied?
   - Temp or staging files that should have been cleaned up still present?

3. **Storage pressure**
   - Disk free on data volumes â€” warn under 20 GB, alert under 5 GB
   - Log files growing unusually fast?
   - Any output directories that have grown more than 1 GB since last check?

4. **Database health** (lightweight, no queries)
   - Connection config files modified recently?
   - Any backup files that look overdue?

5. **System resources**
   - Processes with unusually high CPU or memory? (`ps aux | sort -rk 3 | head -10`)
   - Any zombie processes?

## Response format:

Pipeline healthy â†’ `HEARTBEAT_OK`

Issues â†’
- ðŸ”´ Data loss risk or job failure: immediate attention
- ðŸŸ¡ Performance or storage: address soon
- ðŸŸ¢ Informational: worth knowing
- Include log file snippets (last 5 lines) when flagging errors â€” helps diagnose fast

## Hard rules:
- Do not run, restart, or kill any jobs or processes without instruction
- Do not drop, truncate, or modify any database tables
- Do not delete log or temp files without instruction
- Do not modify pipeline config or cron entries
